{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi all,\n",
    "\n",
    "This is my first notebook. I've studied a few notebooks on this dataset. I learned a lot from this particular notebook https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets written by Janio Martinez. Especially, I learned that you need to first split train/test, and then do undersampling or over sampling.\n",
    "\n",
    "In this notebook I'll do the following:\n",
    "\n",
    "1. Load and scale the data (there is enough visualization study, I'll ignore that). I know from other studies that the data is clean.\n",
    "2. Split the data.\n",
    "3. Train a VAE only on the fraud cases of the training data\n",
    "4. use VAE to generate as many data necessary to match non-fraud case. build an augmented data\n",
    "5. train an ensemble classifier\n",
    "6. test on out-of-sample using different metrics\n",
    "7. wait for comments and critics.\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a few packages, classifiers, metrics and etc."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, average_precision_score, precision_recall_curve\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to put everything in a function for easier handling of data. I also make sure that the output is treated uniformly across experiments.\n",
    "\n",
    "Here is what I'm doing in this function:\n",
    "1. load the data\n",
    "2. scale using StandardScaler in sklearn\n",
    "3. go through each column except the 'Class' and scale it. Drop the column and replace it with a new name that has a prefix 'scaled_'\n",
    "4. Put everything in the X data except 'Class' column that goes to y\n",
    "5. split data\n",
    "6. return as dataframe"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def prepare_data(path, random_state=42):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    path (str): The path to the input data set\n",
    "    random_state (int): A seed to set the random state of the split\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The train data set\n",
    "    pandas.DataFrame: The test data set\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    std_scaler = StandardScaler()\n",
    "    columns = df.columns.drop(['Class'])\n",
    "    for col in columns:\n",
    "        df['scaled_' + str(col)] = std_scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    return pd.concat((xtrain, ytrain), axis=1), pd.concat((xtest, ytest), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple function to return the X value of the fraud case for train and test data"
   ],
   "outputs": []
  },
  {
   "source": [
    "def fraud_or_not(dftrain, dftest):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    dftrain (pandas.DataFrame): The train data frame\n",
    "    dftest (pandas.DataFrame): The test data frame\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The fraud portion of the train data frame\n",
    "    pandas.DataFrame: The fraud portion of the test data frame\n",
    "    \"\"\"\n",
    "    fraud = dftrain.loc[dftrain['Class'] == 1].sample(frac=1)\n",
    "    fraudtest = dftest.loc[dftest['Class'] == 1].sample(frac=1)\n",
    "    return fraud.drop('Class', axis=1), fraudtest.drop('Class', axis=1)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, we want to define our VAE class and a training function for the VAE. Read the inline comments."
   ],
   "outputs": []
  },
  {
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Flatten, Reshape\n",
    "from tensorflow.keras import Sequential\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# This code is taken from Tensorflow tutorial on VAEs. It's turned from a CNN to a simple neuralnet which is more appropriate for our case here.\n",
    "\n",
    "# almost the standard for activation these days\n",
    "relu = tf.nn.relu\n",
    "\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A VAE class inhereted from keras.Model\n",
    "\n",
    "    parameters:\n",
    "    ndim (int): number of dimensions of the input data\n",
    "    latent_dim (int): number of dimensions of the latent variable\n",
    "\n",
    "    attributes:\n",
    "    ndim (int): number of dimensions of the input\n",
    "    latent_dim (int): number of dimensions of the latent variable\n",
    "    inference_net (keras.Sequential): The inference model that takes an input of size=(None, ndim) and return a matrix of size=(None, latent_dim)\n",
    "    generative_net (keras.Sequential): The generative model that takes an input of size=(None, latent_dim) and return a matrix of size=(None, ndim)\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim  \n",
    "        self.ndim = ndim        \n",
    "        self.inference_net = Sequential(\n",
    "            [\n",
    "                InputLayer(input_shape=(ndim,)),\n",
    "                Dense(100, activation=relu),\n",
    "                Dense(2 * latent_dim)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.generative_net = Sequential(\n",
    "            [\n",
    "                InputLayer(input_shape=(latent_dim,)),\n",
    "                Dense(100, activation=relu),\n",
    "                Dense(ndim)\n",
    "            ])\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, num_samples=100, eps=None):\n",
    "        \"\"\"\n",
    "        Given an input noise of size (num_samples, latent_dim), generate samples of size (num_samples, ndim)\n",
    "\n",
    "        parameters:\n",
    "        num_samples (int): number of samples\n",
    "        eps (numpy.ndarray): input noise. if specified, num_samples is ignored\n",
    "\n",
    "        returns:\n",
    "        numpy.ndarray: the decoded samples\n",
    "        \"\"\"\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(num_samples, self.latent_dim))\n",
    "        return self.decode(eps)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters:\n",
    "        x (numpy.ndarray): the input data with size (None, ndim)\n",
    "\n",
    "        returns:\n",
    "        numpy.ndarray: the mean of the latent variables\n",
    "        numpy.ndarray: the log variance of the latent variables\n",
    "        \"\"\"\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterize the input for backpropagation\n",
    "\n",
    "        parameters:\n",
    "        mean (numpy.ndarray): the mean of the latent variables\n",
    "        logvar (numpy.ndarray): the log variance of the latent variables\n",
    "\n",
    "        returns:\n",
    "        numpy.ndarray: the noise samples from a normal distribution around mean with standard deviation exp(logvar / 2)\n",
    "        \"\"\"\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Given an input noise generates the decoded samples\n",
    "\n",
    "        parameters:\n",
    "        z (numpy.ndarray): the input noise (None, latent_dim)\n",
    "\n",
    "        returns:\n",
    "        numpy.ndarray: the decoded samples of size (None, ndim)\n",
    "        \"\"\"\n",
    "        return self.generative_net(z)\n"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for computing the KL term of Gaussian distribution\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-0.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "                         axis=raxis)\n",
    "\n",
    "# a function to compute the loss of the VAE\n",
    "@tf.function\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    logvar = tf.clip_by_value(logvar, -88., 88.)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    xmean = model.decode(z)\n",
    "    logpx_z = -tf.reduce_sum((x - xmean) ** 2, axis=1)  # ad-hoc l2 loss that is pretty close to log-prob of a gaussian distribution withtout taking into account the variance\n",
    "    logpz = log_normal_pdf(z, 0.0, 0.0)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "# A function that given the model computes the loss, the gradients and apply the parameter update\n",
    "@tf.function\n",
    "def compute_apply_gradients(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is easy and self explanatory. I added a few lines to make sure that I can re-load the model every time we want to."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xtrain, xtest, model=None, load=False, filepath=None):\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "    epochs = 2000\n",
    "    latent_dim = 20\n",
    "    num_train, ndim = xtrain.shape\n",
    "    num_test, _ = xtest.shape\n",
    "    if model is None:\n",
    "        model = VAE(ndim, latent_dim)\n",
    "    if load and filepath is not None:\n",
    "        model.load_weights(filepath=filepath)\n",
    "        return model\n",
    "    else:\n",
    "        batch_size = 32\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(xtrain.values.astype(np.float32)).shuffle(num_train).batch(\n",
    "            batch_size)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(xtest.values.astype(np.float32)).shuffle(num_test).batch(num_test)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            start_time = time.time()\n",
    "            for train_x in train_dataset:\n",
    "                compute_apply_gradients(model, train_x, optimizer)\n",
    "            end_time = time.time()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                loss = tf.keras.metrics.Mean()\n",
    "                for test_x in test_dataset:\n",
    "                    loss(compute_loss(model, test_x))\n",
    "                elbo = -loss.result()\n",
    "                print('Epoch: {}, Test set psudo-ELBO: {}, '\n",
    "                      'time elapse for current epoch {}'.format(epoch, elbo, end_time - start_time))\n",
    "                model.save_weights('saved_models/model_%d_at_%d' % (latent_dim, epoch))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the VAE trained, we can pass it to this function and samples from it to increase the number of fraud cases."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data, model):\n",
    "    num_samples = data['Class'].value_counts()[0] - data['Class'].value_counts()[1]\n",
    "    samples = model.sample(num_samples=num_samples).numpy()\n",
    "    dfnew = pd.DataFrame(samples, columns=data.columns.drop('Class'))\n",
    "    dfnew['Class'] = np.ones(len(samples), dtype=np.int)\n",
    "    dfnew = pd.concat((data, dfnew), ignore_index=True).sample(frac=1)\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data and split it\n",
    "dtrain, dtest = get_scaled_data_splitted(path='creditcard.csv')\n",
    "# get the fraud cases from train and test\n",
    "Xf, Xft = fraud_or_not(dtrain, dtest)\n",
    "# get the traied VAE model\n",
    "model = train(Xf, Xft, load=False, filepath='saved_models/model_20_at_1900')\n",
    "# augment the data using the VAE model\n",
    "augmented = augment_data(dtrain, model)\n",
    "X = augmented.drop('Class', axis=1)\n",
    "y = augmented['Class']\n",
    "Xt = dtest.drop('Class', axis=1)\n",
    "yt = dtest['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the augmented X, y, we are going to train a classifier. Then we going to test it on the held-out test set. Be careful that if you're loading a VAE model, it may contain your test set becase the model is train on a specific train/test split, but upon loading you might get a different split."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"XGBClassifier\": XGBClassifier(),\n",
    "    \"BaggingClassifier\": BaggingClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "N = 10000\n",
    "for key, classifier in classifiers.items():\n",
    "    print('_' * 50)\n",
    "    name = key\n",
    "    print(name)\n",
    "    if exists(name):\n",
    "        print('loading...')\n",
    "        classifier = pickle.load(open(name, 'rb'))\n",
    "        training_score = cross_val_score(classifier, X[:N], y[:N], cv=5)\n",
    "        print(\"Classifiers: \", name, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "    else:\n",
    "        classifier.fit(X, y)\n",
    "        print('validating ...')\n",
    "        training_score = cross_val_score(classifier, X[:N], y[:N], cv=5)\n",
    "        print(\"Classifiers: \", name, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "        pickle.dump(classifier, open(name, 'wb'))\n",
    "    print(classifier.score(X, y))\n",
    "    print(classifier.score(Xt, yt))\n",
    "    y_pred = classifier.predict(Xt)\n",
    "    cm = confusion_matrix(yt, y_pred)\n",
    "    print(cm)\n",
    "    print(classification_report(yt, y_pred))\n",
    "    print(average_precision_score(yt, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_interpolation(data, model):\n",
    "    num_samples = data['Class'].value_counts()[0] - data['Class'].value_counts()[1]\n",
    "    X = data[data['Class'] == 1].drop(['Class'], axis=1)\n",
    "    z, _ = model.encode(X.values.astype(np.float32))\n",
    "    z1 = pd.DataFrame(z).sample(frac=num_samples / len(z), replace=True)\n",
    "    z2 = z1.sample(frac=1)\n",
    "    r = np.random.rand(*z1.shape)\n",
    "    z = r * z1.values + (1 - r) * z2.values\n",
    "    samples = model.decode(z.astype(np.float32)).numpy()\n",
    "    dfnew = pd.DataFrame(samples, columns=data.columns.drop('Class'))\n",
    "    dfnew['Class'] = np.ones(len(samples), dtype=np.int)\n",
    "    dfnew = pd.concat((data, dfnew), ignore_index=True).sample(frac=1)\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data_interpolation(dtrain, model)\n",
    "augmented_interpolate = augment_data_interpolation(dtrain, model)\n",
    "X = augmented_interpolate.drop('Class', axis=1)\n",
    "y = augmented_interpolate['Class']\n",
    "Xt = dtest.drop('Class', axis=1)\n",
    "yt = dtest['Class']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"XGBClassifier\": XGBClassifier(),\n",
    "    \"BaggingClassifier\": BaggingClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "N = 10000\n",
    "for key, classifier in classifiers.items():\n",
    "    print('_' * 50)\n",
    "    name = key\n",
    "    print(name)\n",
    "    if exists(name):\n",
    "        print('loading...')\n",
    "        classifier = pickle.load(open(name, 'rb'))\n",
    "        training_score = cross_val_score(classifier, X[:N], y[:N], cv=5)\n",
    "        print(\"Classifiers: \", name, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "    else:\n",
    "        classifier.fit(X, y)\n",
    "        print('validating ...')\n",
    "        training_score = cross_val_score(classifier, X[:N], y[:N], cv=5)\n",
    "        print(\"Classifiers: \", name, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "        pickle.dump(classifier, open(name, 'wb'))\n",
    "    print(classifier.score(X, y))\n",
    "    print(classifier.score(Xt, yt))\n",
    "    y_pred = classifier.predict(Xt)\n",
    "    cm = confusion_matrix(yt, y_pred)\n",
    "    print(cm)\n",
    "    print(classification_report(yt, y_pred))\n",
    "    print(average_precision_score(yt, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}