{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi all,\n",
    "\n",
    "This is my first notebook. I've studied a few notebooks on this dataset. I learned a lot from this particular notebook https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets written by Janio Martinez. Especially, I learned that you need to first split train/test, and then do undersampling or over sampling.\n",
    "\n",
    "In this notebook I'll do the following:\n",
    "\n",
    "1. Load and scale the data (there is enough visualization study, I'll ignore that). I know from other studies that the data is clean.\n",
    "2. Split the data.\n",
    "3. Train a VAE only on the fraud cases of the training data\n",
    "4. use VAE to generate as many data necessary to match non-fraud case. build an augmented data\n",
    "5. train an ensemble classifier\n",
    "6. test on out-of-sample using different metrics\n",
    "7. wait for comments and critics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a few packages, classifiers, metrics and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, average_precision_score, precision_recall_curve\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to put everything in a function for easier handling of data. I also make sure that the output is treated uniformly across experiments.\n",
    "\n",
    "Here is what I'm doing in this function:\n",
    "1. load the data\n",
    "2. scale using StandardScaler in sklearn\n",
    "3. go through each column except the 'Class' and scale it. Drop the column and replace it with a new name that has a prefix 'scaled_'\n",
    "4. Put everything in the X data except 'Class' column that goes to y\n",
    "5. split data\n",
    "6. return as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def get_scaled_data_splitted():\n",
    "    df = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n",
    "    std_scaler = StandardScaler()\n",
    "    columns = df.columns.drop(['Class'])\n",
    "    for col in columns:\n",
    "        df['scaled_' + str(col)] = std_scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)\n",
    "    return pd.concat((xtrain, ytrain), axis=1), pd.concat((xtest, ytest), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple function to return the X value of the fraud case for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_or_not(dftrain, dftest):\n",
    "    fraud = dftrain.loc[dftrain['Class'] == 1].sample(frac=1)\n",
    "    fraudtest = dftest.loc[dftest['Class'] == 1].sample(frac=1)\n",
    "    return fraud.drop('Class', axis=1), fraudtest.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, we want to define our VAE class and a training function for the VAE. Read the inline comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Flatten, Reshape\n",
    "from tensorflow.keras import Sequential\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# This code is taken from Tensorflow tutorial on VAEs. It's turned from a CNN to a simple neuralnet which is more appropriate for our case here.\n",
    "\n",
    "# almost the standard for activation these days\n",
    "relu = tf.nn.relu\n",
    "\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, ndim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim  # number of latent variables of the VAE\n",
    "        \n",
    "        # a simple neural net with one hidden layer. At the end we produce twice the number of latent variables because we are modeling the mean and variance of the Gaussian random variates.\n",
    "        self.inference_net = Sequential(\n",
    "            [\n",
    "                InputLayer(input_shape=(ndim,)),\n",
    "                Dense(100, activation=relu),\n",
    "                Dense(2 * latent_dim)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # A simple decoder network at produces the mean of the variables. An improvement would be to model the variance as well. The training becomes tricky since the variance can collapse.\n",
    "        self.generative_net = Sequential(\n",
    "            [\n",
    "                InputLayer(input_shape=(latent_dim,)),\n",
    "                Dense(100, activation=relu),\n",
    "                Dense(ndim)\n",
    "            ])\n",
    "\n",
    "    # We need this function to generate samples. Start with random normal noise. pass it through the decoder, and you get your samples\n",
    "    @tf.function\n",
    "    def sample(self, num_samples=100, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(num_samples, self.latent_dim))\n",
    "        return self.decode(eps)\n",
    "\n",
    "    # a function to call the encoder network\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    # a function to reparameterie the encoder for the ease of backpropagation\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "    # a function to call the decoder\n",
    "    def decode(self, z):\n",
    "        return self.generative_net(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for computing the KL term of Gaussian distribution\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-0.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "                         axis=raxis)\n",
    "\n",
    "# a function to compute the loss of the VAE\n",
    "@tf.function\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    logvar = tf.clip_by_value(logvar, -88., 88.)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    xmean = model.decode(z)\n",
    "    logpx_z = -tf.reduce_sum((x - xmean) ** 2, axis=1)  # ad-hoc l2 loss that is pretty close to log-prob of a gaussian distribution withtout taking into account the variance\n",
    "    logpz = log_normal_pdf(z, 0.0, 0.0)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "# A function that given the model computes the loss, the gradients and apply the parameter update\n",
    "@tf.function\n",
    "def compute_apply_gradients(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is easy and self explanatory. I added a few lines to make sure that I can re-load the model every time we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xtrain, xtest, model=None, load=False, filepath=None):\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "    epochs = 2000\n",
    "    latent_dim = 20\n",
    "    num_train, ndim = xtrain.shape\n",
    "    num_test, _ = xtest.shape\n",
    "    if model is None:\n",
    "        model = VAE(ndim, latent_dim)\n",
    "    if load and filepath is not None:\n",
    "        model.load_weights(filepath=filepath)\n",
    "        return model\n",
    "    else:\n",
    "        batch_size = 32\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(xtrain.values.astype(np.float32)).shuffle(num_train).batch(\n",
    "            batch_size)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(xtest.values.astype(np.float32)).shuffle(num_test).batch(num_test)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            start_time = time.time()\n",
    "            for train_x in train_dataset:\n",
    "                compute_apply_gradients(model, train_x, optimizer)\n",
    "            end_time = time.time()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                loss = tf.keras.metrics.Mean()\n",
    "                for test_x in test_dataset:\n",
    "                    loss(compute_loss(model, test_x))\n",
    "                elbo = -loss.result()\n",
    "                print('Epoch: {}, Test set psudo-ELBO: {}, '\n",
    "                      'time elapse for current epoch {}'.format(epoch, elbo, end_time - start_time))\n",
    "                model.save_weights('saved_models/model_%d_at_%d' % (latent_dim, epoch))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the VAE trained, we can pass it to this function and samples from it to increase the number of fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data, model):\n",
    "    num_samples = data['Class'].value_counts()[0] - data['Class'].value_counts()[1]\n",
    "    samples = model.sample(num_samples=num_samples).numpy()\n",
    "    dfnew = pd.DataFrame(samples, columns=data.columns.drop('Class'))\n",
    "    dfnew['Class'] = np.ones(len(samples), dtype=np.int)\n",
    "    dfnew = pd.concat((data, dfnew), ignore_index=True).sample(frac=1)\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Test set psudo-ELBO: -50.271484375, time elapse for current epoch 0.022028684616088867\n",
      "Epoch: 200, Test set psudo-ELBO: -40.77564239501953, time elapse for current epoch 0.02196335792541504\n",
      "Epoch: 300, Test set psudo-ELBO: -38.007659912109375, time elapse for current epoch 0.02331399917602539\n",
      "Epoch: 400, Test set psudo-ELBO: -33.243804931640625, time elapse for current epoch 0.023506879806518555\n",
      "Epoch: 500, Test set psudo-ELBO: -31.718900680541992, time elapse for current epoch 0.022860288619995117\n",
      "Epoch: 600, Test set psudo-ELBO: -30.620738983154297, time elapse for current epoch 0.023627042770385742\n",
      "Epoch: 700, Test set psudo-ELBO: -32.56962966918945, time elapse for current epoch 0.020328044891357422\n",
      "Epoch: 800, Test set psudo-ELBO: -29.3978328704834, time elapse for current epoch 0.0248868465423584\n",
      "Epoch: 900, Test set psudo-ELBO: -29.8550968170166, time elapse for current epoch 0.020946979522705078\n",
      "Epoch: 1000, Test set psudo-ELBO: -29.949703216552734, time elapse for current epoch 0.020899057388305664\n",
      "Epoch: 1100, Test set psudo-ELBO: -29.236806869506836, time elapse for current epoch 0.021083354949951172\n",
      "Epoch: 1200, Test set psudo-ELBO: -27.43366050720215, time elapse for current epoch 0.020784378051757812\n",
      "Epoch: 1300, Test set psudo-ELBO: -28.482166290283203, time elapse for current epoch 0.020723819732666016\n",
      "Epoch: 1400, Test set psudo-ELBO: -29.290132522583008, time elapse for current epoch 0.021322011947631836\n",
      "Epoch: 1500, Test set psudo-ELBO: -28.689414978027344, time elapse for current epoch 0.020182132720947266\n",
      "Epoch: 1600, Test set psudo-ELBO: -28.52389144897461, time elapse for current epoch 0.021600723266601562\n",
      "Epoch: 1700, Test set psudo-ELBO: -28.352821350097656, time elapse for current epoch 0.019610166549682617\n",
      "Epoch: 1800, Test set psudo-ELBO: -28.548263549804688, time elapse for current epoch 0.02117133140563965\n",
      "Epoch: 1900, Test set psudo-ELBO: -29.23260498046875, time elapse for current epoch 0.020821094512939453\n",
      "Epoch: 2000, Test set psudo-ELBO: -27.671342849731445, time elapse for current epoch 0.02102971076965332\n"
     ]
    }
   ],
   "source": [
    "# get the data and split it\n",
    "dtrain, dtest = get_scaled_data_splitted()\n",
    "# get the fraud cases from train and test\n",
    "Xf, Xft = fraud_or_not(dtrain, dtest)\n",
    "# get the traied VAE model\n",
    "model = train(Xf, Xft, load=False, filepath='saved_models/model_20_at_1900')\n",
    "# augment the data using the VAE model\n",
    "augmented = augment_data(dtrain, model)\n",
    "X = augmented.drop('Class', axis=1)\n",
    "y = augmented['Class']\n",
    "Xt = dtest.drop('Class', axis=1)\n",
    "yt = dtest['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the augmented X, y, we are going to train a classifier. Then we going to test it on the held-out test set. Be careful that if you're loading a VAE model, it may contain your test set becase the model is train on a specific train/test split, but upon loading you might get a different split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "XGBClassifier\n",
      "validating ...\n",
      "Classifiers:  XGBClassifier Has a training score of 99.0 % accuracy score\n",
      "0.9999076667384814\n",
      "0.9988588883817282\n",
      "[[56824    54]\n",
      " [   11    73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56878\n",
      "           1       0.57      0.87      0.69        84\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.79      0.93      0.85     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "0.499724419783365\n",
      "__________________________________________________\n",
      "BaggingClassifier\n",
      "validating ...\n",
      "Classifiers:  BaggingClassifier Has a training score of 98.0 % accuracy score\n",
      "0.9996130796660174\n",
      "0.9968224430321969\n",
      "[[56708   170]\n",
      " [   11    73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56878\n",
      "           1       0.30      0.87      0.45        84\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.65      0.93      0.72     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "0.26126502967626425\n",
      "__________________________________________________\n",
      "RandomForestClassifier\n",
      "validating ...\n",
      "Classifiers:  RandomForestClassifier Has a training score of 98.0 % accuracy score\n",
      "1.0\n",
      "0.9983497770443454\n",
      "[[56794    84]\n",
      " [   10    74]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56878\n",
      "           1       0.47      0.88      0.61        84\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.73      0.94      0.81     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "0.4127735062062144\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "    \"XGBClassifier\": XGBClassifier(),\n",
    "    \"BaggingClassifier\": BaggingClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "N = 10000\n",
    "for key, classifier in classifiers.items():\n",
    "    print('_' * 50)\n",
    "    name = key\n",
    "    print(name)\n",
    "    if exists(name):\n",
    "        print('loading...')\n",
    "        classifier = pickle.load(open(name, 'rb'))\n",
    "        training_score = cross_val_score(classifier, X[:N], y[:N], cv=5)\n",
    "        print(\"Classifiers: \", name, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "    else:\n",
    "        classifier.fit(X, y)\n",
    "        print('validating ...')\n",
    "        training_score = cross_val_score(classifier, X[:N], y[:N], cv=5)\n",
    "        print(\"Classifiers: \", name, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "        pickle.dump(classifier, open(name, 'wb'))\n",
    "    print(classifier.score(X, y))\n",
    "    print(classifier.score(Xt, yt))\n",
    "    y_pred = classifier.predict(Xt)\n",
    "    cm = confusion_matrix(yt, y_pred)\n",
    "    print(cm)\n",
    "    print(classification_report(yt, y_pred))\n",
    "    print(average_precision_score(yt, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}